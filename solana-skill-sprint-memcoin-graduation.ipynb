{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97569,"databundleVersionId":11856763,"sourceType":"competition"},{"sourceId":11407081,"sourceType":"datasetVersion","datasetId":7012766}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:40:17.137238Z","iopub.execute_input":"2025-04-27T10:40:17.137571Z","iopub.status.idle":"2025-04-27T10:40:17.229494Z","shell.execute_reply.started":"2025-04-27T10:40:17.137549Z","shell.execute_reply":"2025-04-27T10:40:17.228329Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/pump-fun-graduation-february-2025/chunk_40.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_23.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_18.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_21.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_38.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_37.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_36.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_14.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_6.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_35.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_34.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_32.csv\n/kaggle/input/pump-fun-graduation-february-2025/token_info_onchain_divers.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_17.csv\n/kaggle/input/pump-fun-graduation-february-2025/test_unlabeled.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_8.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_7.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_24.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_41.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_9.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_33.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_26.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_11.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_39.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_10.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_31.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_16.csv\n/kaggle/input/pump-fun-graduation-february-2025/dune_token_info_v2.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_4.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_28.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_25.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_22.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_2.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_30.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_19.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_12.csv\n/kaggle/input/pump-fun-graduation-february-2025/dune_token_info.csv\n/kaggle/input/pump-fun-graduation-february-2025/train.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_29.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_15.csv\n/kaggle/input/pump-fun-graduation-february-2025/token_info_onchain_divers_v2.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_1.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_20.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_13.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_27.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_5.csv\n/kaggle/input/pump-fun-graduation-february-2025/chunk_3.csv\n/kaggle/input/solana-skill-sprint-memcoin-graduation/sample_submission.csv\n/kaggle/input/solana-skill-sprint-memcoin-graduation/test_unlabeled.csv\n/kaggle/input/solana-skill-sprint-memcoin-graduation/train.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Cell 1: Setup and Imports\nimport pandas as pd\nimport numpy as np\nimport os # Useful for navigating files, especially in Kaggle environments\n\n# Set options for better display\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:30:28.114571Z","iopub.execute_input":"2025-04-27T10:30:28.115524Z","iopub.status.idle":"2025-04-27T10:30:28.120889Z","shell.execute_reply.started":"2025-04-27T10:30:28.115486Z","shell.execute_reply":"2025-04-27T10:30:28.119557Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Cell 2: Load Data\nBASE_DIR = '/kaggle/input/solana-skill-sprint-memcoin-graduation/'\n\ntrain_df = pd.read_csv(f'{BASE_DIR}train.csv')\ntest_df = pd.read_csv(f'{BASE_DIR}test_unlabeled.csv')\nsample_submission_df = pd.read_csv(f'{BASE_DIR}sample_submission.csv')\n\nprint(\"Train data shape:\", train_df.shape)\nprint(\"Test data shape:\", test_df.shape)\nprint(\"Sample submission shape:\", sample_submission_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:31:48.666607Z","iopub.execute_input":"2025-04-27T10:31:48.667014Z","iopub.status.idle":"2025-04-27T10:31:52.312032Z","shell.execute_reply.started":"2025-04-27T10:31:48.666988Z","shell.execute_reply":"2025-04-27T10:31:52.310749Z"}},"outputs":[{"name":"stdout","text":"Train data shape: (639557, 6)\nTest data shape: (478832, 4)\nSample submission shape: (478832, 2)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 3: Initial Data Inspection\n\nprint(\"\\n--- Train Data Info ---\")\ntrain_df.info()\n\nprint(\"\\n--- Train Data Head ---\")\nprint(train_df.head())\n\nprint(\"\\n--- Train Data Description ---\")\nprint(train_df.describe())\n\nprint(\"\\n--- Test Data Info ---\")\ntest_df.info()\n\nprint(\"\\n--- Test Data Head ---\")\nprint(test_df.head())\n\nprint(\"\\n--- Test Data Description ---\")\nprint(test_df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:32:13.336810Z","iopub.execute_input":"2025-04-27T10:32:13.338095Z","iopub.status.idle":"2025-04-27T10:32:13.528846Z","shell.execute_reply.started":"2025-04-27T10:32:13.338041Z","shell.execute_reply":"2025-04-27T10:32:13.527947Z"}},"outputs":[{"name":"stdout","text":"\n--- Train Data Info ---\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 639557 entries, 0 to 639556\nData columns (total 6 columns):\n #   Column          Non-Null Count   Dtype  \n---  ------          --------------   -----  \n 0   Unnamed: 0      639557 non-null  int64  \n 1   mint            639557 non-null  object \n 2   slot_min        639557 non-null  int64  \n 3   slot_graduated  7404 non-null    float64\n 4   has_graduated   639557 non-null  bool   \n 5   is_valid        639557 non-null  bool   \ndtypes: bool(2), float64(1), int64(2), object(1)\nmemory usage: 20.7+ MB\n\n--- Train Data Head ---\n   Unnamed: 0                                          mint   slot_min  \\\n0           1  BmTDA5HqcemLkEgpyK25sDhbvk652CTXjdWEa8fLpump  317876496   \n1           3  4FJwryCAMMePNeWw9LTBXXfXABdkKcAxkTT6h9pdpump  317876500   \n2           4  DeXchZLMzFm9nfvkfTKiXvBWWEpGMF6VDzvbMB6mpump  317876500   \n3           5  6Mx4fnEPWpRLKtxY8VzVXo3CWwu2dvGp9BSJMWfVpump  317876500   \n4           6  4rbGH5peYV3FvCfncfsfwtBNwpjiG36nZtvxoPZ9pump  317876500   \n\n   slot_graduated  has_graduated  is_valid  \n0             NaN          False      True  \n1             NaN          False      True  \n2             NaN          False      True  \n3             NaN          False      True  \n4             NaN          False      True  \n\n--- Train Data Description ---\n          Unnamed: 0      slot_min  slot_graduated\ncount  639557.000000  6.395570e+05    7.404000e+03\nmean   443022.944580  3.193927e+08    3.194608e+08\nstd    230612.292544  8.661946e+05    8.723438e+05\nmin         1.000000  3.178765e+08    3.178766e+08\n25%    249515.000000  3.186584e+08    3.186939e+08\n50%    451408.000000  3.193919e+08    3.195730e+08\n75%    642602.000000  3.201396e+08    3.201449e+08\nmax    827248.000000  3.209212e+08    3.237205e+08\n\n--- Test Data Info ---\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 478832 entries, 0 to 478831\nData columns (total 4 columns):\n #   Column      Non-Null Count   Dtype \n---  ------      --------------   ----- \n 0   Unnamed: 0  478832 non-null  int64 \n 1   mint        478832 non-null  object\n 2   slot_min    478832 non-null  int64 \n 3   is_valid    478832 non-null  bool  \ndtypes: bool(1), int64(2), object(1)\nmemory usage: 11.4+ MB\n\n--- Test Data Head ---\n   Unnamed: 0                                          mint   slot_min  \\\n0           0  9Wt3N7etKMX9cioTdEJ5S4b8A9nK3M66n9RFVgBGpump  320921219   \n1           1  9q5y2X2P8ZEKTjyXBVcS5q2EZM7HbNV8DURY2qnvqi2f  320921220   \n2           2  HL2di8dcQ7eYDmkcFoZ4zJyHX5SbRZXAJxTegL3JPfx2  320921222   \n3           3  7iAFj9Pc5QH9jbGmHwYe8T6yzNVbjhL13PNJXVTspump  320921224   \n4           4  F7U1Rdgz2KFpneKpAnYytWF2jggnsrLScfi2A668pump  320921228   \n\n   is_valid  \n0      True  \n1      True  \n2      True  \n3      True  \n4      True  \n\n--- Test Data Description ---\n          Unnamed: 0      slot_min\ncount  478832.000000  4.788320e+05\nmean   239415.500000  3.222981e+08\nstd    138227.036385  8.569090e+05\nmin         0.000000  3.209212e+08\n25%    119707.750000  3.215554e+08\n50%    239415.500000  3.222148e+08\n75%    359123.250000  3.230202e+08\nmax    478831.000000  3.239659e+08\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 4: Data Cleaning and Filtering\n\n# Drop the redundant 'Unnamed: 0' column\ntrain_df = train_df.drop('Unnamed: 0', axis=1)\ntest_df = test_df.drop('Unnamed: 0', axis=1)\n\n# Filter data where is_valid is True\n# It's crucial to understand what 'is_valid' means. Assuming it means valid data for the competition.\ntrain_df_valid = train_df[train_df['is_valid']].drop('is_valid', axis=1).copy()\ntest_df_valid = test_df[test_df['is_valid']].drop('is_valid', axis=1).copy() # Keep 'mint' for submission\n\nprint(f\"Original train shape: {train_df.shape}\")\nprint(f\"Filtered train shape: {train_df_valid.shape}\")\nprint(f\"Original test shape: {test_df.shape}\")\nprint(f\"Filtered test shape: {test_df_valid.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:33:24.793803Z","iopub.execute_input":"2025-04-27T10:33:24.794255Z","iopub.status.idle":"2025-04-27T10:33:24.952950Z","shell.execute_reply.started":"2025-04-27T10:33:24.794226Z","shell.execute_reply":"2025-04-27T10:33:24.951868Z"}},"outputs":[{"name":"stdout","text":"Original train shape: (639557, 5)\nFiltered train shape: (639557, 4)\nOriginal test shape: (478832, 3)\nFiltered test shape: (422180, 2)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 5: Target Variable Analysis\n\ntarget_distribution = train_df_valid['has_graduated'].value_counts(normalize=True)\n\nprint(\"\\n--- Target Variable Distribution (has_graduated) ---\")\nprint(target_distribution)\n\n# Optional: Visualize the distribution\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# plt.figure(figsize=(6, 4))\n# sns.countplot(x='has_graduated', data=train_df_valid)\n# plt.title('Distribution of Target Variable (has_graduated)')\n# plt.show()\n\n# Check the number of positive cases\nnum_graduated = train_df_valid['has_graduated'].sum()\nprint(f\"\\nNumber of graduated tokens: {num_graduated}\")\nprint(f\"Percentage of graduated tokens: {target_distribution.get(True, 0) * 100:.4f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:34:24.954257Z","iopub.execute_input":"2025-04-27T10:34:24.954616Z","iopub.status.idle":"2025-04-27T10:34:24.973481Z","shell.execute_reply.started":"2025-04-27T10:34:24.954573Z","shell.execute_reply":"2025-04-27T10:34:24.972381Z"}},"outputs":[{"name":"stdout","text":"\n--- Target Variable Distribution (has_graduated) ---\nhas_graduated\nFalse    0.988423\nTrue     0.011577\nName: proportion, dtype: float64\n\nNumber of graduated tokens: 7404\nPercentage of graduated tokens: 1.1577%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Cell 6: Load and Inspect Auxiliary Data (Corrected)\n\n# Define the base directory for the auxiliary dataset\nAUX_DATA_DIR = '/kaggle/input/pump-fun-graduation-february-2025/'\n\n# List all files in the auxiliary data directory\naux_file_list = os.listdir(AUX_DATA_DIR)\nprint(\"Files in auxiliary data directory:\", aux_file_list)\n\n# Identify transaction data chunks\nchunk_files = sorted([f for f in aux_file_list if f.startswith('chunk') and f.endswith('.csv')])\nprint(f\"\\nFound {len(chunk_files)} transaction data chunks.\")\n\n# Load the first chunk to inspect its structure\nif chunk_files:\n    first_chunk_path = f'{AUX_DATA_DIR}{chunk_files[0]}'\n    print(f\"\\nLoading and inspecting the first chunk file: {first_chunk_path}\")\n    # Read only a few rows and specific columns for quick inspection\n    try:\n        chunk_sample_df = pd.read_csv(first_chunk_path, nrows=100) # Read first 100 rows\n        print(\"\\n--- First Chunk Data Info (Sample) ---\")\n        chunk_sample_df.info()\n        print(\"\\n--- First Chunk Data Head ---\")\n        print(chunk_sample_df.head())\n    except Exception as e:\n        print(f\"Error reading chunk file {first_chunk_path}: {e}\")\n\nelse:\n    print(\"\\nNo chunk files found in the auxiliary directory.\")\n\n\n# Load metadata files\ndune_info_path = f'{AUX_DATA_DIR}dune_token_info.csv'\ntoken_info_divers_path = f'{AUX_DATA_DIR}token_info_onchain_divers.csv'\n\nprint(f\"\\nLoading and inspecting Dune token info: {dune_info_path}\")\ntry:\n    dune_info_df = pd.read_csv(dune_info_path)\n    print(\"\\n--- Dune Token Info Info ---\")\n    dune_info_df.info()\n    print(\"\\n--- Dune Token Info Head ---\")\n    print(dune_info_df.head())\nexcept Exception as e:\n     print(f\"Error reading Dune info file: {e}\")\n\n\nprint(f\"\\nLoading and inspecting Onchain token info: {token_info_divers_path}\")\ntry:\n    token_info_divers_df = pd.read_csv(token_info_divers_path)\n    print(\"\\n--- Onchain Token Info Info ---\")\n    token_info_divers_df.info()\n    print(\"\\n--- Onchain Token Info Head ---\")\n    token_info_divers_df.head() # Use head() as info() was printed just above\nexcept Exception as e:\n    print(f\"Error reading Onchain info file: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:41:40.475080Z","iopub.execute_input":"2025-04-27T10:41:40.475482Z","iopub.status.idle":"2025-04-27T10:42:00.539129Z","shell.execute_reply.started":"2025-04-27T10:41:40.475455Z","shell.execute_reply":"2025-04-27T10:42:00.537966Z"}},"outputs":[{"name":"stdout","text":"Files in auxiliary data directory: ['chunk_40.csv', 'chunk_23.csv', 'chunk_18.csv', 'chunk_21.csv', 'chunk_38.csv', 'chunk_37.csv', 'chunk_36.csv', 'chunk_14.csv', 'chunk_6.csv', 'chunk_35.csv', 'chunk_34.csv', 'chunk_32.csv', 'token_info_onchain_divers.csv', 'chunk_17.csv', 'test_unlabeled.csv', 'chunk_8.csv', 'chunk_7.csv', 'chunk_24.csv', 'chunk_41.csv', 'chunk_9.csv', 'chunk_33.csv', 'chunk_26.csv', 'chunk_11.csv', 'chunk_39.csv', 'chunk_10.csv', 'chunk_31.csv', 'chunk_16.csv', 'dune_token_info_v2.csv', 'chunk_4.csv', 'chunk_28.csv', 'chunk_25.csv', 'chunk_22.csv', 'chunk_2.csv', 'chunk_30.csv', 'chunk_19.csv', 'chunk_12.csv', 'dune_token_info.csv', 'train.csv', 'chunk_29.csv', 'chunk_15.csv', 'token_info_onchain_divers_v2.csv', 'chunk_1.csv', 'chunk_20.csv', 'chunk_13.csv', 'chunk_27.csv', 'chunk_5.csv', 'chunk_3.csv']\n\nFound 41 transaction data chunks.\n\nLoading and inspecting the first chunk file: /kaggle/input/pump-fun-graduation-february-2025/chunk_1.csv\n\n--- First Chunk Data Info (Sample) ---\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100 entries, 0 to 99\nData columns (total 15 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   block_time                   100 non-null    object\n 1   slot                         100 non-null    int64 \n 2   tx_idx                       100 non-null    int64 \n 3   signing_wallet               100 non-null    object\n 4   direction                    100 non-null    object\n 5   base_coin                    100 non-null    object\n 6   base_coin_amount             100 non-null    int64 \n 7   quote_coin_amount            100 non-null    int64 \n 8   virtual_token_balance_after  100 non-null    int64 \n 9   virtual_sol_balance_after    100 non-null    int64 \n 10  signature                    100 non-null    object\n 11  provided_gas_fee             100 non-null    int64 \n 12  provided_gas_limit           100 non-null    int64 \n 13  fee                          100 non-null    int64 \n 14  consumed_gas                 100 non-null    int64 \ndtypes: int64(10), object(5)\nmemory usage: 11.8+ KB\n\n--- First Chunk Data Head ---\n            block_time       slot  tx_idx  \\\n0  2025-02-01 16:00:00  317876496    1897   \n1  2025-02-01 16:00:00  317876496    1794   \n2  2025-02-01 16:00:00  317876496    1880   \n3  2025-02-01 16:00:01  317876499    1350   \n4  2025-02-01 16:00:01  317876500    2389   \n\n                                 signing_wallet direction  \\\n0  EXoaGQc1taATjsXVPXhnVYddW4KiM1uQRhrYDdfi1x7b       buy   \n1  9Ypu1cMva6dE6k9Zk4aSSmSgJvMmJLTWXuGQhTYqt8mx       buy   \n2  3njxeVx5TjDYD27C1YsZW2JQzgmoYeATambcbw7Xn1ft      sell   \n3  DjZ1Cpxp6uKvYHU678QkjFj8XKfUwAmCtxUMG5QuBdJT       buy   \n4  6WgXuHPo9xWu1Mzt8hULYWFwpyh1WwhByPYjsF23h41A       buy   \n\n                                      base_coin  base_coin_amount  \\\n0   Ab2voNJxp9xM2sdoF6JRJV8dtZ6hGm8yMSt3xAMpump     5208861189189   \n1  BmTDA5HqcemLkEgpyK25sDhbvk652CTXjdWEa8fLpump     1785357737104   \n2  FZ8wX1RAwV72gniwc9quiZSXHnrECQwoCxAXCWRipump      721068391933   \n3  BmTDA5HqcemLkEgpyK25sDhbvk652CTXjdWEa8fLpump   115855160976852   \n4  4FJwryCAMMePNeWw9LTBXXfXABdkKcAxkTT6h9pdpump    67062499999999   \n\n   quote_coin_amount  virtual_token_balance_after  virtual_sol_balance_after  \\\n0          150740503             1052069532604495                30596837025   \n1           50000000             1071214642262896                30050000000   \n2           22610289             1013561711601751                31759289915   \n3         3644123135              955359481286044                33694123135   \n4         2000000000             1005937500000001                32000000000   \n\n                                           signature  provided_gas_fee  \\\n0  3Td5mZpy63TNuyHncgpJFvPudbU3fKcRaeTbqTRsdSLKnN...           3982833   \n1  3HHvJsNKWg6epToaZUouqDJdkysiJGKeBkWnhg7sPyWpFY...           9475209   \n2  2rfHemwRWv9t2xuY2umAq2aQrx8Gn73g6tAEDBTE2yrzcf...           4624039   \n3  Ky5DURUWgB7N3NfNAx5jf7a23pecQAwxbcwuwaWE2MPk7R...         170888000   \n4  4rSq4SK7a6zb2hNV6x89DfwZ9XFAyy4sp1g9YQfuG8A3Ep...            100000   \n\n   provided_gas_limit       fee  consumed_gas  \n0              200862    805000        161488  \n1              194994   1857609        194994  \n2              162000    754095         78540  \n3               80000  13676040         62135  \n4              500000     60000        201798  \n\nLoading and inspecting Dune token info: /kaggle/input/pump-fun-graduation-february-2025/dune_token_info.csv\n\n--- Dune Token Info Info ---\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 947474 entries, 0 to 947473\nData columns (total 7 columns):\n #   Column              Non-Null Count   Dtype \n---  ------              --------------   ----- \n 0   token_mint_address  947474 non-null  object\n 1   decimals            947474 non-null  int64 \n 2   name                942203 non-null  object\n 3   symbol              941999 non-null  object\n 4   token_uri           942454 non-null  object\n 5   created_at          947474 non-null  object\n 6   init_tx             947474 non-null  object\ndtypes: int64(1), object(6)\nmemory usage: 50.6+ MB\n\n--- Dune Token Info Head ---\n                             token_mint_address  decimals              name  \\\n0  4zTPCRE1P9VwdtUeqVLZ9T9TQzRrP2DDUpSUDkyKpump         6         Burp Fish   \n1  B9neSjRGFtZ7tkw53mh7TEyD9BiEi5gpvTHKttKepump         6           3 Biter   \n2  9aUWB9Pf7e32XPq6Cqj3Lbjh23yep6nb47yZnDtXpump         6              fugu   \n3  DaGDgkjuN7kSU4CdtZdqVe2DA2hcpXmACh2t7L1Cpump         6  Trumps First Dog   \n4  9nxbaRi1utDeVF3r6CoFSrda6bfV3d4TwgsqP2HUpump         6          DDC Coin   \n\n   symbol                                          token_uri  \\\n0    BURP  https://ipfs.io/ipfs/QmWmphgkU2bZR7XqZBLpvZrJS...   \n1    taco  https://ipfs.io/ipfs/QmdhQdNyaDDrvLwGTxy3PfYkM...   \n2    FUGU  https://ipfs.io/ipfs/QmNhjoKpdUiY1saU71HR3xx3C...   \n3  PATTON  https://ipfs.io/ipfs/QmdBvaS441sXnDiPcsA9qottZ...   \n4     DDC  https://ipfs.io/ipfs/QmYUoAuLz998Q1G7vbqAkZbJV...   \n\n                    created_at  \\\n0  2025-02-12 10:30:11.000 UTC   \n1  2025-02-24 07:23:17.000 UTC   \n2  2025-02-24 07:24:46.000 UTC   \n3  2025-02-14 06:09:23.000 UTC   \n4  2025-02-22 20:33:57.000 UTC   \n\n                                             init_tx  \n0  2rcmr3xLLL9rkyr8L9myUbog2FnpVBqCWGvihS1VK2TKHE...  \n1  2Pis9BdJ34enoe2sqQdFz1bsGW7X2eNyAmubbqGG7grfzR...  \n2  4TMsc2VkPy9PLgJrBa4MGuJhtaNB3p8YdKydPhCVzDwhYo...  \n3  5nSooX5iGexxVHcobhh1RXbaeXwjnSiyVEdomUtGffWHDN...  \n4  2TTKZSeRHjZNc257mu7xeFU3DeGHNYN86HwxXADr83bd29...  \n\nLoading and inspecting Onchain token info: /kaggle/input/pump-fun-graduation-february-2025/token_info_onchain_divers.csv\n\n--- Onchain Token Info Info ---\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1710051 entries, 0 to 1710050\nData columns (total 15 columns):\n #   Column                Dtype  \n---  ------                -----  \n 0   block_time            object \n 1   slot                  object \n 2   tx_idx                object \n 3   creator               object \n 4   name                  object \n 5   symbol                object \n 6   url                   object \n 7   mint                  object \n 8   bundle_size           float64\n 9   gas_used              float64\n 10  bundled_buys          float64\n 11  bundled_buys_count    float64\n 12  dev_balance           float64\n 13  creation_ix_index     float64\n 14  direct_pf_invocation  float64\ndtypes: float64(7), object(8)\nmemory usage: 195.7+ MB\n\n--- Onchain Token Info Head ---\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/2405911188.py:49: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n  token_info_divers_df = pd.read_csv(token_info_divers_path)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Cell 7: Feature Engineering Strategy - Transaction Data\n\n# This cell outlines the strategy, the implementation will follow in later cells.\n\nprint(\"--- Feature Engineering Strategy from Transaction Data ---\")\n\n# The transaction data (chunk*.csv) contains records for many tokens.\n# We need to join/merge this data with our train_df_valid and test_df_valid\n# using the 'mint' column and potentially 'slot_min' to filter by time.\n\n# For each unique 'mint' in our train and test sets, we will process the transaction data\n# within its first 100 blocks (relative to 'slot_min').\n\n# Potential features to extract per 'mint' from transaction data:\n# 1. Transaction Counts:\n#    - Total number of transactions (buys + sells)\n#    - Number of buy transactions\n#    - Number of sell transactions\n# 2. Volume:\n#    - Total SOL volume (in/out)\n#    - Total token volume (in/out)\n#    - Net SOL flow (SOL bought - SOL sold)\n#    - Net Token flow (Tokens bought - Tokens sold)\n# 3. Participants:\n#    - Number of unique wallets participating (buyers + sellers)\n#    - Number of unique buyers\n#    - Number of unique sellers\n# 4. Price/Volatility:\n#    - Price at different points (e.g., start, end of 100 blocks, max, min)\n#    - Price change over the 100 blocks\n#    - Volatility (e.g., standard deviation of price within the window)\n# 5. Balances:\n#    - Final virtual SOL balance (might indicate remaining liquidity)\n#    - Final virtual token balance\n\n# Challenge: Processing potentially large chunk files efficiently.\n# Strategy:\n# A. Read chunks one by one.\n# B. For each chunk, filter transactions that belong to the 'mint' addresses\n#    in our train/test sets AND occurred within the first 100 blocks\n#    after that mint's 'slot_min'.\n# C. Aggregate features for each 'mint' from the filtered transactions.\n# D. Concatenate aggregated features for all chunks.\n# E. Merge the resulting feature dataframe with train_df_valid and test_df_valid.\n\nprint(\"\\nNext steps will involve implementing this strategy: iterating through chunks, filtering, aggregating, and merging.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:45:37.311075Z","iopub.execute_input":"2025-04-27T10:45:37.311429Z","iopub.status.idle":"2025-04-27T10:45:37.318986Z","shell.execute_reply.started":"2025-04-27T10:45:37.311405Z","shell.execute_reply":"2025-04-27T10:45:37.317473Z"}},"outputs":[{"name":"stdout","text":"--- Feature Engineering Strategy from Transaction Data ---\n\nNext steps will involve implementing this strategy: iterating through chunks, filtering, aggregating, and merging.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Cell 8: Implement Feature Extraction for a Single Chunk (Corrected Filtering Logic)\n\ndef extract_features_from_chunk(chunk_path, tokens_info_df):\n    \"\"\"\n    Reads a transaction chunk file, filters relevant transactions for the tokens\n    in tokens_info_df within their first 100 slots, and extracts features.\n    Uses corrected column names and filtering logic.\n\n    Args:\n        chunk_path (str): The full path to the chunk CSV file.\n        tokens_info_df (pd.DataFrame): DataFrame containing 'mint' and 'slot_min'\n                                      for the tokens we care about (from train/test).\n\n    Returns:\n        pd.DataFrame: A DataFrame with 'mint' as index and aggregated features.\n                      Returns an empty DataFrame if no relevant transactions found.\n    \"\"\"\n    # print(f\"Processing chunk: {os.path.basename(chunk_path)}\") # Keep this for debugging progress\n\n    # Read the chunk file - reading in chunks might be necessary for very large files\n    try:\n        # Specify relevant dtypes to potentially reduce memory usage and avoid warnings\n        dtype_mapping = {\n            'slot': 'int64',\n            'tx_idx': 'int64', # Based on previous info, might need handling if mixed types, sticking to int for now\n            'signing_wallet': 'object',\n            'direction': 'object',\n            'base_coin': 'object', # This is the mint address\n            'base_coin_amount': 'int64', # Token amount\n            'quote_coin_amount': 'int64', # SOL amount\n            'virtual_token_balance_after': 'int64',\n            'virtual_sol_balance_after': 'int64',\n            'provided_gas_fee': 'int64',\n            'fee': 'int64',\n            'consumed_gas': 'int64'\n            # block_time and signature can remain objects if not used for calculations\n        }\n        # Only load columns needed for merging and feature extraction\n        cols_to_load = ['slot', 'tx_idx', 'signing_wallet', 'direction', 'base_coin',\n                        'base_coin_amount', 'quote_coin_amount',\n                        'virtual_token_balance_after', 'virtual_sol_balance_after'] # Added tx_idx for sorting\n        chunk_df = pd.read_csv(chunk_path, dtype=dtype_mapping, usecols=cols_to_load)\n    except Exception as e:\n        print(f\"Error reading {chunk_path}: {e}\")\n        return pd.DataFrame() # Return empty if read fails\n\n    # Rename 'base_coin' to 'mint' to match the tokens_info_df and facilitate merging\n    chunk_df = chunk_df.rename(columns={'base_coin': 'mint'})\n\n    # Merge with tokens_info_df to get slot_min for filtering\n    # We only keep transactions for the mints in our train/test sets\n    merged_df = pd.merge(chunk_df, tokens_info_df[['mint', 'slot_min']], on='mint', how='inner')\n\n    if merged_df.empty:\n        # print(\"No relevant transactions found in this chunk.\")\n        return pd.DataFrame()\n\n    # Filter transactions within the first 100 slots for each mint\n    # CORRECTED FILTERING LOGIC HERE\n    filtered_df = merged_df[\n        (merged_df['slot'] >= merged_df['slot_min']) &\n        (merged_df['slot'] < merged_df['slot_min'] + 100)\n    ].copy() # Use .copy() to avoid SettingWithCopyWarning\n\n\n    if filtered_df.empty:\n        # print(\"No transactions within the first 100 slots in this chunk.\")\n        return pd.DataFrame()\n\n    # Sort by slot and tx_idx to ensure correct 'first'/'last' for aggregation\n    filtered_df = filtered_df.sort_values(by=['slot', 'tx_idx']).reset_index(drop=True)\n\n\n    # --- Feature Aggregation ---\n    # Group by 'mint' and calculate features\n    features_df = filtered_df.groupby('mint').agg(\n        n_tx=('slot', 'count'), # Number of transactions\n        n_buys=('direction', lambda x: (x == 'buy').sum()), # Number of buy transactions\n        n_sells=('direction', lambda x: (x == 'sell').sum()), # Number of sell transactions\n        # Corrected volume calculations based on direction and amount columns\n        buy_sol_volume=('quote_coin_amount', lambda x: x[filtered_df.loc[x.index, 'direction'] == 'buy'].sum()), # SOL spent on buys\n        sell_sol_volume=('quote_coin_amount', lambda x: x[filtered_df.loc[x.index, 'direction'] == 'sell'].sum()), # SOL received from sells\n        buy_token_volume=('base_coin_amount', lambda x: x[filtered_df.loc[x.index, 'direction'] == 'buy'].sum()), # Tokens received from buys\n        sell_token_volume=('base_coin_amount', lambda x: x[filtered_df.loc[x.index, 'direction'] == 'sell'].sum()), # Tokens spent on sells\n\n        # Note: Net flow calculations might need review depending on desired definition\n        # For simplicity, let's calculate total SOL/Token traded regardless of direction for now\n        # Or sum based on buy/sell volumes calculated above\n        total_sol_volume = ('quote_coin_amount', 'sum'), # Sum of quote_coin_amount (SOL)\n        total_token_volume = ('base_coin_amount', 'sum'), # Sum of base_coin_amount (Tokens)\n\n        unique_wallets=('signing_wallet', 'nunique'), # Number of unique wallets in this chunk/window\n\n        # Price approximations: SOL per Token\n        # Use total volumes for a more stable price estimate within the window\n        total_buy_sol_agg=('quote_coin_amount', lambda x: x[filtered_df.loc[x.index, 'direction'] == 'buy'].sum()),\n        total_buy_token_agg=('base_coin_amount', lambda x: x[filtered_df.loc[x.index, 'direction'] == 'buy'].sum()),\n\n        # Virtual balance at the end of the window (or last recorded transaction)\n        # 'last' aggregation works correctly after sorting by slot/tx_idx\n        last_virtual_sol_balance=('virtual_sol_balance_after', 'last'),\n        last_virtual_token_balance=('virtual_token_balance_after', 'last'),\n\n        # First/Last slot in the window for this mint within this chunk\n        first_tx_slot=('slot', 'min'),\n        last_tx_slot=('slot', 'max')\n\n    ).reset_index() # Get 'mint' back as a column\n\n    # Recalculate average price using aggregated total volumes\n    features_df['avg_price_buy'] = features_df.apply(\n        lambda row: row['total_buy_sol_agg'] / row['total_buy_token_agg'] if row['total_buy_token_agg'] != 0 else 0, axis=1\n    )\n    features_df = features_df.drop(['total_buy_sol_agg', 'total_buy_token_agg'], axis=1) # Drop intermediate columns\n\n    # Add prefix to features to indicate source\n    features_df = features_df.rename(columns={col: f'tx_{col}' for col in features_df.columns if col != 'mint'})\n\n\n    return features_df\n\n# Prepare the tokens_info_df by combining train and test mints and slot_mins\n# Use the filtered dataframes from Cell 4\ntokens_info_for_feature_extraction = pd.concat([\n    train_df_valid[['mint', 'slot_min']],\n    test_df_valid[['mint', 'slot_min']]\n]).drop_duplicates(subset=['mint']).reset_index(drop=True)\n\nprint(f\"\\nPrepared token info for feature extraction ({len(tokens_info_for_feature_extraction)} unique mints).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:57:39.724538Z","iopub.execute_input":"2025-04-27T10:57:39.724980Z","iopub.status.idle":"2025-04-27T10:57:40.022628Z","shell.execute_reply.started":"2025-04-27T10:57:39.724954Z","shell.execute_reply":"2025-04-27T10:57:40.021274Z"}},"outputs":[{"name":"stdout","text":"\nPrepared token info for feature extraction (1061737 unique mints).\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Cell 9: Process All Chunks and Aggregate Features (Execute Again)\n\n# List to hold feature dataframes from each chunk\nall_features_list = []\n\n# Get the list of chunk files again (assuming Cell 6 was run)\naux_file_list = os.listdir(AUX_DATA_DIR)\nchunk_files = sorted([f for f in aux_file_list if f.startswith('chunk') and f.endswith('.csv')])\n\nprint(f\"Starting feature extraction from {len(chunk_files)} chunks...\")\n\n# Iterate through each chunk file\nfor i, chunk_file in enumerate(chunk_files):\n    chunk_path = f'{AUX_DATA_DIR}{chunk_file}'\n    # Call the function to extract features\n    chunk_features_df = extract_features_from_chunk(chunk_path, tokens_info_for_feature_extraction)\n    if not chunk_features_df.empty:\n        all_features_list.append(chunk_features_df)\n    # Optional: Print progress\n    if (i + 1) % 5 == 0 or (i + 1) == len(chunk_files):\n        print(f\"Finished processing {i + 1}/{len(chunk_files)} chunks.\")\n    # Optional: Add a small sleep to avoid overwhelming the system or for monitoring\n    # import time\n    # time.sleep(1)\n\n\nprint(\"\\nFinished processing all chunks. Concatenating results...\")\n\n# Concatenate all feature dataframes\nif all_features_list:\n    transaction_features_df = pd.concat(all_features_list, ignore_index=True)\n    print(\"Concatenation complete.\")\n    print(\"Transaction features shape (before cross-chunk aggregation):\", transaction_features_df.shape)\n\n    # Group by mint again in case a mint appeared in multiple chunks (likely)\n    # Aggregate features across chunks for the same mint\n    print(\"Aggregating features across chunks for mints...\")\n    # Define aggregation methods for each column\n    agg_dict = {\n        col: 'sum' for col in transaction_features_df.columns if col.startswith('tx_') and col not in ['tx_last_virtual_sol_balance', 'tx_last_virtual_token_balance', 'tx_first_tx_slot', 'tx_last_tx_slot', 'tx_avg_price_buy']\n    }\n    # For price, avg of averages is not ideal, but sum of SOL / sum of Tokens is better\n    # Since we already calculated avg_price_buy per chunk, we can take the mean, or recalculate\n    # based on total buy sol/token across chunks if needed.\n    # Let's stick to mean for now, or perhaps re-evaluate price calculation later.\n    agg_dict['tx_avg_price_buy'] = 'mean' # Or potentially re-calculate after summing volumes\n    agg_dict['tx_last_virtual_sol_balance'] = 'last' # Last recorded balance across all relevant transactions for the mint\n    agg_dict['tx_last_virtual_token_balance'] = 'last' # Last recorded balance across all relevant transactions for the mint\n    agg_dict['tx_first_tx_slot'] = 'min' # Earliest tx slot across all chunks for the mint\n    agg_dict['tx_last_tx_slot'] = 'max' # Latest tx slot across all chunks for the mint\n\n    transaction_features_df = transaction_features_df.groupby('mint').agg(agg_dict).reset_index()\n    print(\"Aggregation across chunks complete.\")\n    print(\"Transaction features shape after cross-chunk aggregation:\", transaction_features_df.shape)\n\n\n    print(\"\\n--- Transaction Features Head ---\")\n    print(transaction_features_df.head())\n    print(\"\\n--- Transaction Features Info ---\")\n    transaction_features_df.info()\n    # print(\"\\n--- Transaction Features Description ---\") # Might be too verbose\n    # print(transaction_features_df.describe())\n\n\nelse:\n    transaction_features_df = pd.DataFrame()\n    print(\"No transaction features extracted.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T10:57:52.092925Z","iopub.execute_input":"2025-04-27T10:57:52.093286Z","iopub.status.idle":"2025-04-27T11:51:09.827986Z","shell.execute_reply.started":"2025-04-27T10:57:52.093262Z","shell.execute_reply":"2025-04-27T11:51:09.826614Z"}},"outputs":[{"name":"stdout","text":"Starting feature extraction from 41 chunks...\nFinished processing 5/41 chunks.\nFinished processing 10/41 chunks.\nFinished processing 15/41 chunks.\nFinished processing 20/41 chunks.\nFinished processing 25/41 chunks.\nFinished processing 30/41 chunks.\nFinished processing 35/41 chunks.\nFinished processing 40/41 chunks.\nFinished processing 41/41 chunks.\n\nFinished processing all chunks. Concatenating results...\nConcatenation complete.\nTransaction features shape (before cross-chunk aggregation): (1062257, 16)\nAggregating features across chunks for mints...\nAggregation across chunks complete.\nTransaction features shape after cross-chunk aggregation: (1061737, 16)\n\n--- Transaction Features Head ---\n                                          mint  tx_n_tx  tx_n_buys  \\\n0  112GXdaoTJN4nrL2P2Gc3vXKSkHmTfzvLMBGJukpump       61         36   \n1  114Y1wSoqDdPZpcb8kDedZ1KU1bvmVXAJdNidGRpump       10          6   \n2  115Xxvg1WzerrEwjgDaVL93uULKko8Z1QLDMD5Epump       90         54   \n3  115vjkpCJMXwikx4trTUSpkHAexJGAWUXQTQjc1E4cj        7          5   \n4  116koTGrrhy691TUR9QnESz28Dr5uHBuhZrgVFFpump       16         10   \n\n   tx_n_sells  tx_buy_sol_volume  tx_sell_sol_volume  tx_buy_token_volume  \\\n0          25        12574649110         11779477708      331951765057375   \n1           4         2608744025          2463282571       85944797606665   \n2          36        37451182171         27176158503      880044608812843   \n3           2         5159453968          3053877815      157456771373502   \n4           6         3743841477          2557865457      124815645859863   \n\n   tx_sell_token_volume  tx_total_sol_volume  tx_total_token_volume  \\\n0       304245509430254          24354126818        636197274487629   \n1        80767230941070           5072026596        166712028547735   \n2       606299261035324          64627340674       1486343869848167   \n3        87086341076330           8213331783        244543112449832   \n4        84010372918728           6301706934        208826018778591   \n\n   tx_unique_wallets  tx_avg_price_buy  tx_last_virtual_sol_balance  \\\n0                 36          0.000038                  30795171402   \n1                  6          0.000030                  30145461454   \n2                 54          0.000043                  40275023668   \n3                  5          0.000033                  32105576153   \n4                 10          0.000030                  31185976020   \n\n   tx_last_virtual_token_balance  tx_first_tx_slot  tx_last_tx_slot  \n0               1045293744372879         317972522        317972613  \n1               1067822433334405         319310748        319310826  \n2                799254652222481         323029899        323029992  \n3               1002629569702828         319774019        319774081  \n4               1032194727058865         322238847        322238942  \n\n--- Transaction Features Info ---\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1061737 entries, 0 to 1061736\nData columns (total 16 columns):\n #   Column                         Non-Null Count    Dtype  \n---  ------                         --------------    -----  \n 0   mint                           1061737 non-null  object \n 1   tx_n_tx                        1061737 non-null  int64  \n 2   tx_n_buys                      1061737 non-null  int64  \n 3   tx_n_sells                     1061737 non-null  int64  \n 4   tx_buy_sol_volume              1061737 non-null  int64  \n 5   tx_sell_sol_volume             1061737 non-null  int64  \n 6   tx_buy_token_volume            1061737 non-null  int64  \n 7   tx_sell_token_volume           1061737 non-null  int64  \n 8   tx_total_sol_volume            1061737 non-null  int64  \n 9   tx_total_token_volume          1061737 non-null  int64  \n 10  tx_unique_wallets              1061737 non-null  int64  \n 11  tx_avg_price_buy               1061737 non-null  float64\n 12  tx_last_virtual_sol_balance    1061737 non-null  int64  \n 13  tx_last_virtual_token_balance  1061737 non-null  int64  \n 14  tx_first_tx_slot               1061737 non-null  int64  \n 15  tx_last_tx_slot                1061737 non-null  int64  \ndtypes: float64(1), int64(14), object(1)\nmemory usage: 129.6+ MB\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Cell 10: Merge Features and Prepare Data for Modeling (Corrected Column Names for NaN Fill)\n\n# Merge transaction features into train and test dataframes\nprint(\"Merging transaction features with train and test data...\")\n\n# Left merge to keep all original train/test tokens, even those with no transactions\ntrain_merged_df = pd.merge(train_df_valid, transaction_features_df, on='mint', how='left')\ntest_merged_df = pd.merge(test_df_valid, transaction_features_df, on='mint', how='left')\n\nprint(f\"Train merged shape: {train_merged_df.shape}\")\nprint(f\"Test merged shape: {test_merged_df.shape}\")\n\n# Identify columns with NaN values introduced by the merge (mints with no transactions)\n# Make sure transaction_features_df is not empty before getting columns\nif not transaction_features_df.empty:\n    new_feature_columns = transaction_features_df.columns.difference(['mint'])\n    print(f\"\\nNew feature columns added: {list(new_feature_columns)}\")\n\n    # Check for NaNs in the new feature columns\n    print(\"\\nNaN counts in new features (Train):\")\n    print(train_merged_df[new_feature_columns].isnull().sum().sort_values(ascending=False).head())\n\n    print(\"\\nNaN counts in new features (Test):\")\n    print(test_merged_df[new_feature_columns].isnull().sum().sort_values(ascending=False).head())\n\n    # Handle NaNs - A common approach is imputation (e.g., filling with 0 for count/volume features)\n    # For count/volume features, NaN likely means 0 activity in the first 100 blocks.\n    # For balance features, NaN might need a different imputation or handling.\n    # Let's fill most new features with 0.\n    # CORRECTED: Use the prefixed column names for balance features\n    balance_cols = ['tx_last_virtual_sol_balance', 'tx_last_virtual_token_balance']\n    features_to_fill_zero = new_feature_columns.difference(balance_cols)\n\n    train_merged_df[features_to_fill_zero] = train_merged_df[features_to_fill_zero].fillna(0)\n    test_merged_df[features_to_fill_zero] = test_merged_df[features_to_fill_zero].fillna(0)\n\n    # CORRECTED: Use the prefixed column names for balance features when filling NaNs\n    train_merged_df[balance_cols] = train_merged_df[balance_cols].fillna(0)\n    test_merged_df[balance_cols] = test_merged_df[balance_cols].fillna(0)\n\nelse:\n     print(\"\\nNo transaction features dataframe to merge.\")\n     # If transaction_features_df is empty, train_merged_df and test_merged_df are just train_df_valid and test_df_valid\n     train_merged_df = train_df_valid.copy()\n     test_merged_df = test_df_valid.copy()\n\n\n# Define features (X) and target (y)\nTARGET = 'has_graduated'\n# Drop mint, slot_min, slot_graduated (if present in train), and any other non-feature columns\nfeatures = [col for col in train_merged_df.columns if col not in [TARGET, 'mint', 'slot_min', 'slot_graduated']]\n\nX_train = train_merged_df[features]\ny_train = train_merged_df[TARGET]\nX_test = test_merged_df[features] # We need mint for submission later, but not for prediction. Keep test_merged_df for submission.\n\nprint(f\"\\nFeatures for training ({len(features)}):\", features)\nprint(f\"\\nShape of X_train: {X_train.shape}\")\nprint(f\"Shape of y_train: {y_train.shape}\")\nprint(f\"Shape of X_test: {X_test.shape}\")\n\n# Display head of prepared data\nprint(\"\\n--- X_train Head ---\")\nprint(X_train.head())\n\nprint(\"\\n--- X_test Head ---\")\nprint(X_test.head())\n\n# Store the test_merged_df for later submission\ntest_merged_df_for_submission = test_merged_df.copy()\n\n# Note: Metadata features from dune_token_info and token_info_onchain_divers\n# are not included yet. That would be another feature engineering step if desired.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T11:54:22.941568Z","iopub.execute_input":"2025-04-27T11:54:22.941997Z","iopub.status.idle":"2025-04-27T11:54:26.170417Z","shell.execute_reply.started":"2025-04-27T11:54:22.941973Z","shell.execute_reply":"2025-04-27T11:54:26.169267Z"}},"outputs":[{"name":"stdout","text":"Merging transaction features with train and test data...\nTrain merged shape: (639557, 19)\nTest merged shape: (422180, 17)\n\nNew feature columns added: ['tx_avg_price_buy', 'tx_buy_sol_volume', 'tx_buy_token_volume', 'tx_first_tx_slot', 'tx_last_tx_slot', 'tx_last_virtual_sol_balance', 'tx_last_virtual_token_balance', 'tx_n_buys', 'tx_n_sells', 'tx_n_tx', 'tx_sell_sol_volume', 'tx_sell_token_volume', 'tx_total_sol_volume', 'tx_total_token_volume', 'tx_unique_wallets']\n\nNaN counts in new features (Train):\ntx_avg_price_buy       0\ntx_buy_sol_volume      0\ntx_buy_token_volume    0\ntx_first_tx_slot       0\ntx_last_tx_slot        0\ndtype: int64\n\nNaN counts in new features (Test):\ntx_avg_price_buy       0\ntx_buy_sol_volume      0\ntx_buy_token_volume    0\ntx_first_tx_slot       0\ntx_last_tx_slot        0\ndtype: int64\n\nFeatures for training (15): ['tx_n_tx', 'tx_n_buys', 'tx_n_sells', 'tx_buy_sol_volume', 'tx_sell_sol_volume', 'tx_buy_token_volume', 'tx_sell_token_volume', 'tx_total_sol_volume', 'tx_total_token_volume', 'tx_unique_wallets', 'tx_avg_price_buy', 'tx_last_virtual_sol_balance', 'tx_last_virtual_token_balance', 'tx_first_tx_slot', 'tx_last_tx_slot']\n\nShape of X_train: (639557, 15)\nShape of y_train: (639557,)\nShape of X_test: (422180, 15)\n\n--- X_train Head ---\n   tx_n_tx  tx_n_buys  tx_n_sells  tx_buy_sol_volume  tx_sell_sol_volume  \\\n0        3          2           1         3694123135          1962505810   \n1        1          1           0         2000000000                   0   \n2        3          2           1         1787482158           787482157   \n3        1          1           0         4000000000                   0   \n4       13          8           5         4390651881          3484311162   \n\n   tx_buy_token_volume  tx_sell_token_volume  tx_total_sol_volume  \\\n0      117640518713956        59086132098194           5656628945   \n1       67062499999999                     0           2000000000   \n2       60337221608106        25724318382300           2574964315   \n3      126235294117647                     0           4000000000   \n4      137117631913714       105651480735568           7874963043   \n\n   tx_total_token_volume  tx_unique_wallets  tx_avg_price_buy  \\\n0        176726650812150                  2          0.000031   \n1         67062499999999                  1          0.000030   \n2         86061539990406                  2          0.000030   \n3        126235294117647                  1          0.000032   \n4        242769112649282                  8          0.000032   \n\n   tx_last_virtual_sol_balance  tx_last_virtual_token_balance  \\\n0                  31731617325               1014445613384238   \n1                  32000000000               1005937500000001   \n2                  31000000001               1038387096774194   \n3                  34000000000                946764705882353   \n4                  30906340719               1041533848821854   \n\n   tx_first_tx_slot  tx_last_tx_slot  \n0         317876496        317876549  \n1         317876500        317876500  \n2         317876500        317876512  \n3         317876500        317876500  \n4         317876500        317876594  \n\n--- X_test Head ---\n   tx_n_tx  tx_n_buys  tx_n_sells  tx_buy_sol_volume  tx_sell_sol_volume  \\\n0        5          4           1         1574777319            61821346   \n1       13          8           5         2543364926          2479069001   \n2       23         15           8         9698244054          6112562106   \n3        4          3           1         2793477335          1075677334   \n4        9          6           3         1595093511            91185279   \n\n   tx_buy_token_volume  tx_sell_token_volume  tx_total_sol_volume  \\\n0       53515375433138         2000000000000           1636598665   \n1       84007404389013        81712671747529           5022433927   \n2      283637401333863       183075212684781          15810806160   \n3       91402358747737        33289898237776           3869154669   \n4       54171562154954         3045335525475           1686278790   \n\n   tx_total_token_volume  tx_unique_wallets  tx_avg_price_buy  \\\n0         55515375433138                  4          0.000029   \n1        165720076136542                  8          0.000030   \n2        466712614018644                 14          0.000034   \n3        124692256985513                  3          0.000031   \n4         57216897680429                  6          0.000029   \n\n   tx_last_virtual_sol_balance  tx_last_virtual_token_balance  \\\n0                  31512955973               1021484624566862   \n1                  30064295925               1070705267358516   \n2                  30261025706               1063744511536750   \n3                  31717800001               1014887539490039   \n4                  31000925163               1038356108239952   \n\n   tx_first_tx_slot  tx_last_tx_slot  \n0         320921219        320921253  \n1         320921220        320921311  \n2         320921222        320921320  \n3         320921224        320921239  \n4         320921228        320921288  \n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Cell 11: Model Selection\n\nimport lightgbm as lgb\n# import xgboost as xgb # XGBoost is another strong alternative\n\nprint(\"Selecting LightGBM Classifier as the model.\")\n# We will instantiate the model in the next cell where we train it,\n# potentially setting parameters like objective, metrics, and class_weight.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T11:55:19.646373Z","iopub.execute_input":"2025-04-27T11:55:19.647444Z","iopub.status.idle":"2025-04-27T11:55:27.182753Z","shell.execute_reply.started":"2025-04-27T11:55:19.647414Z","shell.execute_reply":"2025-04-27T11:55:27.181673Z"}},"outputs":[{"name":"stdout","text":"Selecting LightGBM Classifier as the model.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Cell 12: Model Training\n\n# Calculate scale_pos_weight for handling imbalance\nneg_count = y_train.value_counts().get(False, 0)\npos_count = y_train.value_counts().get(True, 0)\nscale_pos_weight_value = neg_count / pos_count if pos_count > 0 else 1\n\nprint(f\"Calculated scale_pos_weight: {scale_pos_weight_value}\")\n\n# Initialize the LightGBM Classifier model\n# Using parameters often found effective in similar competitions.\n# 'objective': 'binary' for binary classification\n# 'metric': 'logloss' as per the competition evaluation metric\n# 'n_estimators': Number of boosting rounds (can be tuned)\n# 'learning_rate': Step size shrinkage (can be tuned)\n# 'num_leaves': Complexity of the tree (can be tuned)\n# 'seed': for reproducibility\n# 'n_jobs': for parallel processing\n# 'scale_pos_weight': to handle class imbalance\n\nlgb_clf = lgb.LGBMClassifier(objective='binary',\n                             metric='logloss',\n                             n_estimators=1000, # A starting point, consider early stopping\n                             learning_rate=0.05,\n                             num_leaves=31, # Default, can be tuned\n                             max_depth=-1, # No limit\n                             random_state=42,\n                             n_jobs=-1,\n                             scale_pos_weight=scale_pos_weight_value, # Address imbalance\n                             colsample_bytree=0.8, # Feature fraction\n                             subsample=0.8, # Data fraction\n                             reg_alpha=0.1, # L1 regularization\n                             reg_lambda=0.1 # L2 regularization\n                            )\n\nprint(\"Starting model training...\")\n\n# Train the model\nlgb_clf.fit(X_train, y_train)\n\nprint(\"Model training complete.\")\n\n# Note: For better evaluation and to prevent overfitting,\n# cross-validation and early stopping should be used in practice.\n# This is a basic training step for now.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T11:55:50.194541Z","iopub.execute_input":"2025-04-27T11:55:50.195430Z","iopub.status.idle":"2025-04-27T11:56:13.130812Z","shell.execute_reply.started":"2025-04-27T11:55:50.195394Z","shell.execute_reply":"2025-04-27T11:56:13.130036Z"}},"outputs":[{"name":"stdout","text":"Calculated scale_pos_weight: 85.37992976769314\nStarting model training...\n[LightGBM] [Info] Number of positive: 7404, number of negative: 632153\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094233 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3745\n[LightGBM] [Info] Number of data points in the train set: 639557, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011577 -> initscore=-4.447111\n[LightGBM] [Info] Start training from score -4.447111\nModel training complete.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Cell 13: Make Predictions\n\nprint(\"Making predictions on the test set...\")\n\n# Predict probabilities. The output is an array of shape (n_samples, 2),\n# where the second column ([[:, 1]]) is the probability of the positive class (True, i.e., graduated).\ntest_probabilities = lgb_clf.predict_proba(X_test)[:, 1]\n\nprint(\"Predictions complete.\")\nprint(\"Shape of test probabilities:\", test_probabilities.shape)\nprint(\"Sample test probabilities:\", test_probabilities[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T11:59:12.439086Z","iopub.execute_input":"2025-04-27T11:59:12.440170Z","iopub.status.idle":"2025-04-27T11:59:22.555073Z","shell.execute_reply.started":"2025-04-27T11:59:12.440136Z","shell.execute_reply":"2025-04-27T11:59:22.553926Z"}},"outputs":[{"name":"stdout","text":"Making predictions on the test set...\nPredictions complete.\nShape of test probabilities: (422180,)\nSample test probabilities: [0.30599323 0.02892616 0.11633548 0.25011308 0.16772698 0.25895146\n 0.08822064 0.10529137 0.03042206 0.32733643]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Cell 14: Generate Submission File\n\nprint(\"Generating submission file...\")\n\n# Create a submission DataFrame\n# The sample submission has columns 'mint' and 'has_graduated'.\nsubmission_df = pd.DataFrame({'mint': test_merged_df_for_submission['mint'],\n                              'has_graduated': test_probabilities})\n\n# Save the submission file\nsubmission_path = 'submission.csv'\nsubmission_df.to_csv(submission_path, index=False)\n\nprint(f\"Submission file created successfully: {submission_path}\")\nprint(\"\\n--- Submission File Head ---\")\nprint(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T12:00:05.740922Z","iopub.execute_input":"2025-04-27T12:00:05.742180Z","iopub.status.idle":"2025-04-27T12:00:07.310497Z","shell.execute_reply.started":"2025-04-27T12:00:05.742133Z","shell.execute_reply":"2025-04-27T12:00:07.309126Z"}},"outputs":[{"name":"stdout","text":"Generating submission file...\nSubmission file created successfully: submission.csv\n\n--- Submission File Head ---\n                                           mint  has_graduated\n0  9Wt3N7etKMX9cioTdEJ5S4b8A9nK3M66n9RFVgBGpump       0.305993\n1  9q5y2X2P8ZEKTjyXBVcS5q2EZM7HbNV8DURY2qnvqi2f       0.028926\n2  HL2di8dcQ7eYDmkcFoZ4zJyHX5SbRZXAJxTegL3JPfx2       0.116335\n3  7iAFj9Pc5QH9jbGmHwYe8T6yzNVbjhL13PNJXVTspump       0.250113\n4  F7U1Rdgz2KFpneKpAnYytWF2jggnsrLScfi2A668pump       0.167727\n","output_type":"stream"}],"execution_count":28}]}