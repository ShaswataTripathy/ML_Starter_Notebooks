{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shaswatatripathy/store-sales-prediction?scriptVersionId=232013110\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:35:02.74781Z","iopub.execute_input":"2025-04-05T07:35:02.748059Z","iopub.status.idle":"2025-04-05T07:35:03.919602Z","shell.execute_reply.started":"2025-04-05T07:35:02.748028Z","shell.execute_reply":"2025-04-05T07:35:03.91839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Environment Setup & Library Imports","metadata":{}},{"cell_type":"code","source":"# Cell 1: Environment Setup & Library Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For model building, we might use LightGBM (or XGBoost later)\nimport lightgbm as lgb\n\n# Set global visualization style\nsns.set(style=\"whitegrid\")\nplt.style.use(\"seaborn-whitegrid\")\n\n# Display all columns in DataFrame outputs\npd.set_option('display.max_columns', None)\n\nprint(\"Libraries imported successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:35:16.517893Z","iopub.execute_input":"2025-04-05T07:35:16.518238Z","iopub.status.idle":"2025-04-05T07:35:20.898876Z","shell.execute_reply.started":"2025-04-05T07:35:16.518211Z","shell.execute_reply":"2025-04-05T07:35:20.897892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"# Cell 2: Data Loading\n\n# Define the data directory if running in Kaggle\n# Adjust the paths based on the dataset structure on Kaggle\ndata_path = \"../input/store-sales-time-series-forecasting/\"\n\n# Load datasets\ntrain_df = pd.read_csv(data_path + \"train.csv\", parse_dates=['date'])\ntest_df = pd.read_csv(data_path + \"test.csv\", parse_dates=['date'])\nstores_df = pd.read_csv(data_path + \"stores.csv\")\noil_df = pd.read_csv(data_path + \"oil.csv\", parse_dates=['date'])\nholidays_df = pd.read_csv(data_path + \"holidays_events.csv\", parse_dates=['date'])\ntransactions_df = pd.read_csv(data_path + \"transactions.csv\", parse_dates=['date'])\n\n# Quick look at the data shapes and a sample of each\nprint(\"Train data shape:\", train_df.shape)\nprint(train_df.head())\n\nprint(\"\\nTest data shape:\", test_df.shape)\nprint(test_df.head())\n\nprint(\"\\nStores data shape:\", stores_df.shape)\nprint(stores_df.head())\n\nprint(\"\\nOil data shape:\", oil_df.shape)\nprint(oil_df.head())\n\nprint(\"\\nHolidays & Events data shape:\", holidays_df.shape)\nprint(holidays_df.head())\n\nprint(\"\\nTransactions data shape:\", transactions_df.shape)\nprint(transactions_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:40:57.003953Z","iopub.execute_input":"2025-04-05T07:40:57.004335Z","iopub.status.idle":"2025-04-05T07:40:59.223289Z","shell.execute_reply.started":"2025-04-05T07:40:57.00431Z","shell.execute_reply":"2025-04-05T07:40:59.222003Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# Cell 3: Exploratory Data Analysis (EDA) & Data Cleaning\n\n# Check for missing values in each DataFrame\ndata_frames = {\n    'Train': train_df,\n    'Test': test_df,\n    'Stores': stores_df,\n    'Oil': oil_df,\n    'Holidays': holidays_df,\n    'Transactions': transactions_df\n}\n\nfor name, df in data_frames.items():\n    print(f\"Missing values in {name} DataFrame:\")\n    print(df.isnull().sum())\n    print(\"-\" * 50)\n\n# Summary statistics for the training data\nprint(\"Summary statistics for train data:\")\nprint(train_df.describe())\n\n# Distribution of sales in the training data\nplt.figure(figsize=(10, 6))\nsns.histplot(train_df['sales'], bins=50, kde=True)\nplt.title(\"Distribution of Sales\")\nplt.xlabel(\"Sales\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# Time series trend: Total sales aggregated by date\nsales_by_date = train_df.groupby('date')['sales'].sum().reset_index()\nplt.figure(figsize=(14, 7))\nsns.lineplot(data=sales_by_date, x='date', y='sales')\nplt.title(\"Total Sales Over Time\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Total Sales\")\nplt.xticks(rotation=45)\nplt.show()\n\n# Analyze the 'onpromotion' feature in the training set\nplt.figure(figsize=(10, 6))\nsns.histplot(train_df['onpromotion'], bins=50, kde=False)\nplt.title(\"Distribution of Onpromotion Counts\")\nplt.xlabel(\"Onpromotion Count\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# Transactions data: Summary statistics and distribution\nprint(\"Summary statistics for transactions data:\")\nprint(transactions_df.describe())\n\nplt.figure(figsize=(10, 6))\nsns.histplot(transactions_df['transactions'], bins=50, kde=True)\nplt.title(\"Distribution of Transactions\")\nplt.xlabel(\"Transactions\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# Total sales by store (aggregated)\nsales_by_store = train_df.groupby('store_nbr')['sales'].sum().reset_index()\nplt.figure(figsize=(10, 6))\nsns.barplot(data=sales_by_store, x='store_nbr', y='sales')\nplt.title(\"Total Sales by Store\")\nplt.xlabel(\"Store Number\")\nplt.ylabel(\"Total Sales\")\nplt.show()\n\n# --- Additional EDA Ideas Implementation ---\n\n# 1. Sales distribution by product family\n# Option A: Boxplot to see the spread of sales by family\nplt.figure(figsize=(14, 7))\nsns.boxplot(data=train_df, x='family', y='sales')\nplt.title(\"Sales Distribution by Product Family\")\nplt.xlabel(\"Product Family\")\nplt.ylabel(\"Sales\")\nplt.xticks(rotation=90)\nplt.show()\n\n# Option B: Bar plot of average sales per family\nfamily_sales = train_df.groupby('family')['sales'].mean().reset_index()\nplt.figure(figsize=(14, 7))\nsns.barplot(data=family_sales, x='family', y='sales')\nplt.title(\"Average Sales by Product Family\")\nplt.xlabel(\"Product Family\")\nplt.ylabel(\"Average Sales\")\nplt.xticks(rotation=90)\nplt.show()\n\n# 2. Analyze oil prices over time\nplt.figure(figsize=(14, 7))\nsns.lineplot(data=oil_df, x='date', y='dcoilwtico')\nplt.title(\"Oil Prices Over Time\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Oil Price (dcoilwtico)\")\nplt.xticks(rotation=45)\nplt.show()\n\n# 3. Look into holiday effects by merging holidays_df with sales data\n# Create an indicator in the training data for whether the date is a holiday\ntrain_df['is_holiday'] = train_df['date'].isin(holidays_df['date'])\nholiday_sales = train_df.groupby('is_holiday')['sales'].sum().reset_index()\n\nplt.figure(figsize=(8, 6))\nsns.barplot(data=holiday_sales, x='is_holiday', y='sales')\nplt.title(\"Total Sales on Holiday vs Non-Holiday Days\")\nplt.xlabel(\"Is Holiday (False=0, True=1)\")\nplt.ylabel(\"Total Sales\")\nplt.show()\n\n# Optional: Compare average sales on holidays vs non-holidays\navg_sales_holiday = train_df.groupby('is_holiday')['sales'].mean().reset_index()\nplt.figure(figsize=(8, 6))\nsns.barplot(data=avg_sales_holiday, x='is_holiday', y='sales')\nplt.title(\"Average Sales on Holiday vs Non-Holiday Days\")\nplt.xlabel(\"Is Holiday (False=0, True=1)\")\nplt.ylabel(\"Average Sales\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:46:21.974289Z","iopub.execute_input":"2025-04-05T07:46:21.974705Z","iopub.status.idle":"2025-04-05T07:46:44.035432Z","shell.execute_reply.started":"2025-04-05T07:46:21.974677Z","shell.execute_reply":"2025-04-05T07:46:44.034505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# feature engineering","metadata":{}},{"cell_type":"code","source":"# Cell 4: Feature Engineering\n\n# --- 1. Combine Train and Test Data ---\n# Flag training vs. test and create a dummy sales column in test\ntrain_df['is_train'] = 1\ntest_df['is_train'] = 0\ntest_df['sales'] = np.nan\n\n# Combine train and test for consistent feature engineering\ncombined_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)\nprint(\"Combined data shape:\", combined_df.shape)\n\n# --- 2. Date Features ---\ncombined_df['year'] = combined_df['date'].dt.year\ncombined_df['month'] = combined_df['date'].dt.month\ncombined_df['day'] = combined_df['date'].dt.day\ncombined_df['dayofweek'] = combined_df['date'].dt.dayofweek\ncombined_df['weekofyear'] = combined_df['date'].dt.isocalendar().week.astype(int)\ncombined_df['quarter'] = combined_df['date'].dt.quarter\n\n# --- 3. Merge External Data ---\n\n# Merge Stores Data\ncombined_df = combined_df.merge(stores_df, on='store_nbr', how='left')\nprint(\"After merging stores, shape:\", combined_df.shape)\n\n# Merge Oil Data: Use ffill and bfill to handle missing values\noil_df['dcoilwtico'] = oil_df['dcoilwtico'].ffill().bfill()\ncombined_df = combined_df.merge(oil_df, on='date', how='left')\nprint(\"After merging oil data, shape:\", combined_df.shape)\n\n# Merge Transactions Data\ncombined_df = combined_df.merge(transactions_df, on=['date', 'store_nbr'], how='left')\nprint(\"After merging transactions, shape:\", combined_df.shape)\n\n# Remove any existing \"is_holiday\" column before merging holidays to avoid duplicates\nif 'is_holiday' in combined_df.columns:\n    combined_df.drop(columns=['is_holiday'], inplace=True)\n\n# Merge Holidays Data:\n# Ensure the holidays dates are parsed with dayfirst=True (format dd-mm-yyyy)\nholidays_df['date'] = pd.to_datetime(holidays_df['date'], dayfirst=True)\n\n# Convert transferred column to boolean if it's not already\nif holidays_df['transferred'].dtype == object:\n    holidays_df['transferred'] = holidays_df['transferred'].map({'FALSE': False, 'TRUE': True})\n\n# Filter holidays where transferred is False (i.e., holiday is celebrated on that day)\nholidays_flag = holidays_df[holidays_df['transferred'] == False][['date']]\nholidays_flag = holidays_flag.drop_duplicates()\n# Rename the date column to avoid merge conflicts\nholidays_flag = holidays_flag.rename(columns={'date': 'holiday_date'})\n# Add a holiday flag\nholidays_flag['is_holiday_flag'] = 1\n\n# Merge the holiday flag using combined_df.date and holidays_flag.holiday_date\ncombined_df = combined_df.merge(holidays_flag[['holiday_date', 'is_holiday_flag']],\n                                left_on='date', right_on='holiday_date', how='left')\n# Fill missing holiday flag values with 0, drop the extra column, and rename the flag column\ncombined_df['is_holiday'] = combined_df['is_holiday_flag'].fillna(0).astype(int)\ncombined_df.drop(columns=['holiday_date', 'is_holiday_flag'], inplace=True)\nprint(\"After merging holidays, shape:\", combined_df.shape)\n\n# --- 4. Lag and Rolling Features ---\n# Sort values to ensure correct lag calculations\ncombined_df.sort_values(by=['store_nbr', 'family', 'date'], inplace=True)\n\n# Create lag features for 'sales' within each store and product family group\ncombined_df['lag_7'] = combined_df.groupby(['store_nbr', 'family'])['sales'].shift(7)\ncombined_df['lag_14'] = combined_df.groupby(['store_nbr', 'family'])['sales'].shift(14)\n\n# Create a 7-day rolling average of past sales (shift by 1 day to avoid leakage)\ncombined_df['rolling_mean_7'] = combined_df.groupby(['store_nbr', 'family'])['sales'].shift(1).rolling(window=7).mean()\n\n# --- 5. Additional Features ---\n# Example: Ratio of onpromotion items to transactions (adding 1 to avoid division by zero)\ncombined_df['promo_ratio'] = combined_df['onpromotion'] / (combined_df['transactions'] + 1)\n\n# Display a sample of the engineered features\nprint(combined_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:13:59.151646Z","iopub.execute_input":"2025-04-05T08:13:59.1521Z","iopub.status.idle":"2025-04-05T08:14:05.295205Z","shell.execute_reply.started":"2025-04-05T08:13:59.152071Z","shell.execute_reply":"2025-04-05T08:14:05.294048Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing & Train-Validation Split","metadata":{}},{"cell_type":"code","source":"# Cell 5: Data Preprocessing & Train-Validation Split\n\n# ----- Handle Missing Values -----\n# Fill missing transactions and oil prices if any remain\ncombined_df['transactions'] = combined_df['transactions'].ffill().bfill()\ncombined_df['dcoilwtico'] = combined_df['dcoilwtico'].ffill().bfill()\n\n# ----- Separate Train and Test Data -----\n# Training data: where is_train==1 and we have a valid sales value\ntrain_data = combined_df[combined_df['is_train'] == 1].copy()\n# Test data: where is_train==0 (sales remains NaN)\ntest_data = combined_df[combined_df['is_train'] == 0].copy()\n\n# ----- Drop Rows with Missing Lag Features in Training Data -----\n# (These typically correspond to the first few days with no available lag data)\ntrain_data = train_data.dropna(subset=['lag_7', 'lag_14', 'rolling_mean_7'])\nprint(\"Training data after dropping rows with missing lag features:\", train_data.shape)\n\n# ----- Convert Relevant Columns to Categorical -----\n# For modeling (e.g., with LightGBM), it's useful to mark categorical features\ncat_cols = ['store_nbr', 'family', 'city', 'state', 'type']\nfor col in cat_cols:\n    train_data[col] = train_data[col].astype('category')\n    test_data[col] = test_data[col].astype('category')\n\n# ----- Create Time-Based Train/Validation Split -----\n# Sort training data by date to maintain time series order\ntrain_data.sort_values('date', inplace=True)\n\n# Define a validation start date (e.g., last 90 days of training data)\nval_start_date = train_data['date'].max() - pd.Timedelta(days=90)\n\n# Split into train and validation sets based on date\ntrain_set = train_data[train_data['date'] < val_start_date].copy()\nval_set   = train_data[train_data['date'] >= val_start_date].copy()\n\nprint(\"Final Training Set Shape:\", train_set.shape)\nprint(\"Validation Set Shape:\", val_set.shape)\nprint(\"Test Set Shape:\", test_data.shape)\n\n# ----- Prepare Feature and Target Lists -----\n# Define the list of features to use (dropping columns not needed for modeling)\n# We'll drop identifiers, date, target (sales), and the is_train flag.\nfeatures = ['store_nbr', 'family', 'onpromotion', 'year', 'month', 'day', \n            'dayofweek', 'weekofyear', 'quarter', 'city', 'state', 'type', \n            'cluster', 'dcoilwtico', 'transactions', 'is_holiday', \n            'lag_7', 'lag_14', 'rolling_mean_7', 'promo_ratio']\n\n# The target variable\ntarget = 'sales'\n\n# Optionally, inspect the features\nprint(\"Feature list for modeling:\", features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:16:00.098049Z","iopub.execute_input":"2025-04-05T08:16:00.098933Z","iopub.status.idle":"2025-04-05T08:16:03.809754Z","shell.execute_reply.started":"2025-04-05T08:16:00.098892Z","shell.execute_reply":"2025-04-05T08:16:03.808293Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training and Evaluation","metadata":{}},{"cell_type":"code","source":"# Cell 6: Model Training and Evaluation\n\nimport lightgbm as lgb\nimport numpy as np\n\n# Define the RMSLE function (for external evaluation if needed)\ndef rmsle(y_true, y_pred):\n    y_pred = np.maximum(0, y_pred)  # Ensure predictions are non-negative\n    return np.sqrt(np.mean((np.log1p(y_pred) - np.log1p(y_true)) ** 2))\n\n# Define a custom evaluation metric for LightGBM\ndef lgb_rmsle(preds, dataset):\n    labels = dataset.get_label()\n    preds = np.maximum(0, preds)  # Ensure non-negative predictions\n    score = np.sqrt(np.mean((np.log1p(preds) - np.log1p(labels)) ** 2))\n    return 'rmsle', score, False  # False indicates that lower is better\n\n# Prepare LightGBM datasets using our feature list and target variable\nlgb_train = lgb.Dataset(train_set[features], label=train_set[target], categorical_feature=cat_cols)\nlgb_val = lgb.Dataset(val_set[features], label=val_set[target], categorical_feature=cat_cols, reference=lgb_train)\n\n# Set LightGBM parameters\nparams = {\n    'objective': 'regression',\n    'metric': 'None',  # We'll use our custom evaluation metric\n    'boosting': 'gbdt',\n    'learning_rate': 0.05,\n    'num_leaves': 31,\n    'verbose': -1,\n    'seed': 42,\n}\n\n# Create a dictionary to record evaluation results using the callback\nevals_result = {}\n\n# Train the model with early stopping using callbacks\nmodel = lgb.train(\n    params,\n    lgb_train,\n    valid_sets=[lgb_train, lgb_val],\n    valid_names=['train', 'valid'],\n    num_boost_round=1000,\n    feval=lgb_rmsle,\n    callbacks=[lgb.record_evaluation(evals_result), lgb.early_stopping(100)]\n)\n\nprint(\"Best iteration:\", model.best_iteration)\nprint(\"Validation RMSLE:\", model.best_score['valid']['rmsle'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:26:38.564644Z","iopub.execute_input":"2025-04-05T08:26:38.565018Z","iopub.status.idle":"2025-04-05T08:30:09.862016Z","shell.execute_reply.started":"2025-04-05T08:26:38.564991Z","shell.execute_reply":"2025-04-05T08:30:09.860983Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate Predictions and Create Submission File","metadata":{}},{"cell_type":"code","source":"# Cell 7: Generate Predictions and Create Submission File\n\n# Generate predictions on the test set using the best iteration of the model\npredictions = model.predict(test_data[features], num_iteration=model.best_iteration)\n\n# Ensure predictions are non-negative (clip negative values to zero)\npredictions = np.maximum(0, predictions)\n\n# Create the submission DataFrame with the required 'id' and 'sales' columns\nsubmission = test_data[['id']].copy()\nsubmission['sales'] = predictions\n\n# Display the first few rows of the submission DataFrame\nprint(submission.head())\n\n# Save the submission file to CSV\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:38:18.192009Z","iopub.execute_input":"2025-04-05T08:38:18.192393Z","iopub.status.idle":"2025-04-05T08:38:18.846016Z","shell.execute_reply.started":"2025-04-05T08:38:18.192368Z","shell.execute_reply":"2025-04-05T08:38:18.844951Z"}},"outputs":[],"execution_count":null}]}